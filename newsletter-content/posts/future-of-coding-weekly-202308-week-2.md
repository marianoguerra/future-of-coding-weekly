<!--
.. title: Future of Coding Weekly 2023/08 Week 2
.. slug: future-of-coding-weekly-202308-week-2
.. date: 2023-08-13 23:09:00 UTC+02:00
.. tags: 
.. category: 
.. link: 
.. description: 
.. type: text
-->

⚙️ Rage Against the State Machine 🚲 Structured Focused Writing 💡 Live, Rich & Composable

# Our Work

🐦 [Nicolae Rusan on Twitter](https://twitter.com/NicolaeRusan/status/1689428120695390208) via **Nicolae Rusan**

[🧵 conversation](https://history.futureofcoding.org/history/weekly/2023/08/W2/share-your-work.html#2023-08-10T01:25:50.554Z)

👋 howdy folks! We just shipped an integration for Linear & Github issues in Magnet ([www.magnet.run](http://www.magnet.run)) our AI coding assistant that is powered by GPT-4.



This gives you a way to quickly take an issue, and go end-to-end on getting suggestion for relevant files, having Magnet ask clarifying questions, and having it make suggestions on how to implement the feature across your entire stack, e.g. front-end, backend, and database.




Lots of fun explorations as we’ve built this, and we’re going to be diving deeper into some of the research questions that have come up in terms of how to give AI appropriate context on the task at hand

> 🐦 [Nicolae Rusan on Twitter](https://twitter.com/NicolaeRusan/status/1689428120695390208): Magnet (<https://t.co/hyTk0nzO28>) now offers Linear &amp; GitHub issue integration:
>
> → Start a workflow from a Linear or GitHub issue
> → Magnet suggests relevant files from your codebase for that issue
> → Magnet poses clarifying questions, guiding you end-to-end on features and bugs
>

🎥 [Magnet Issues Integration - Adding a new feature around meta tags](https://www.loom.com/share/e20772ec32b248ba977bbb9d526c0951?t=1) via **Nicolae Rusan**

[🧵 conversation](https://history.futureofcoding.org/history/weekly/2023/08/W2/share-your-work.html#2023-08-10T01:26:34.265Z)

🎥 Here’s a Loom video of me starting from an issue in Linear and getting to a working feature using Magnet if you want to skip the thread above:

>In this video, I discuss how I am utilizing magnets to enhance the process of shipping new features. I noticed that when sharing a magnet link from our blog, the title tags and images were not updated for the specific blog posts on the blog page. To address this, I demonstrate how to select the relevant files using the linear issues integration and suggest implementing the feature. Additionally, I explore the benefits of using Next SEO and explain why it is a valuable addition. Join me as we work on this feature together!

🤖 [Visual Programming in VSCode: Create Discord Bots the Easy Way](https://www.flyde.dev) via **Gabriel Grinberg**

[🧵 conversation](https://history.futureofcoding.org/history/weekly/2023/08/W2/share-your-work.html#2023-08-12T18:54:32.844Z)

Hey all 👋

I just published a tutorial on how to use Flyde ([flyde.dev](https://www.flyde.dev)) to build a Discord bot - [Visual Programming in VSCode: Create Discord Bots the Easy Way](https://medium.com/@gabrielgrinberg/visual-programming-in-vscode-create-discord-bots-the-easy-way-dcccc6a9528d) - an attempt to provide a real-world use-case to visual flow-based programming 🚀



Looking forward to any feedback or comment 🙏



PS: It's more beginners oriented, as that's the direction I'm taking Flyde at the moment, so expect it to be laid simpler than the usual content here 🙃

![gif5.gif](http://history.futureofcoding.org/history/msg_files/F05/F05M4CMKV1D.gif)

# Devlog Together

💬 [Kartik Agaram](http://akkartik.name/about)

[🧵 conversation](https://history.futureofcoding.org/history/weekly/2023/08/W2/devlog-together.html#2023-08-11T18:09:57.149Z)

Yesterday was a good day. I built a little app for someone to connect to their R server, plot a dataset based on different dimensions. Pretty trivial, but hopefully easier for them to modify than other similar apps out there.


# Thinking Together

🐦 [John Carmack on Twitter](https://twitter.com/ID_AA_Carmack/status/1689297553408315392) via [Mariano Guerra](https://twitter.com/warianoguerra)

[🧵 conversation](https://history.futureofcoding.org/history/weekly/2023/08/W2/thinking-together.html#2023-08-10T09:23:06.827Z)

What do you think about this?








> 🐦 [John Carmack on Twitter](https://twitter.com/ID_AA_Carmack/status/1689297553408315392): Despite all the effort that has gone into it, it doesn’t look like programming language design has any real compounding power. There are better and worse languages, but other market and technical forces can swamp language choices.


⚙️ [State machine: Rplot](https://docs.google.com/document/d/1EbK4AxDCDWonMa8KyGJFX4jllXXLew0qBsGxsmqoYqk) via [Kartik Agaram](http://akkartik.name/about)

[🧵 conversation](https://history.futureofcoding.org/history/weekly/2023/08/W2/thinking-together.html#2023-08-13T16:59:59.321Z)

Last night I spent some time documenting an almost-trivial state machine that I keep getting wrong: [State machine: Rplot](https://docs.google.com/document/d/1EbK4AxDCDWonMa8KyGJFX4jllXXLew0qBsGxsmqoYqk)



State machines are  _hard_ !




* We usually have one in our head when we program.
* The abstract state in my head is usually made up of multiple concrete variables in the program.
* Mutations happen to variables, but it's not obvious at each how the abstract state is changing.
* The state machine in my head often evolves, which makes documentation challenging to keep up to date. Which is why I think nobody builds documents like these.



Tell me how immutability or State Charts will solve all my problems 🙂


# Content

🚲 [Bike Outliner: Structured focused writing app for Mac](https://www.hogbaysoftware.com/bike/) via **Eli Mellen**

[🧵 conversation](https://history.futureofcoding.org/history/weekly/2023/08/W2/linking-together.html#2023-08-07T14:24:36.681Z)

[Is anyone else using Jesse Grosjean’s Bike outliner on mac?](https://www.hogbaysoftware.com/bike/)



As an outliner it is fairly minimal. It can’t hold a candle to something like omnioutliner or org-mode by ways of features. But the text handling and rendering is pretty gorgeous, especially at input.
> 📝 [Bike Outliner: Structured focused writing app for Mac](https://www.hogbaysoftware.com/bike/)
>
>Use Bike to think, write, organize. Make lists, take notes, create documents. Bike's an unusually fast outliner designed for your Mac.

💻 [Project IDX](https://idx.dev/) via [Mattia Fregola](https://twitter.com/MattiaFregola)

[🧵 conversation](https://history.futureofcoding.org/history/weekly/2023/08/W2/linking-together.html#2023-08-09T06:21:15.546Z)

[idx.dev](https://idx.dev)
> 📝 [Project IDX](https://idx.dev/)
>
>Project IDX is an entirely web-based workspace for full-stack application development, complete with the latest generative AI (powered by Codey and PaLM 2), and full-fidelity app previews, powered by cloud emulators.



📝 [Who Can Name the Bigger Number?](https://www.scottaaronson.com/writings/bignumbers.html) via **Eli Mellen**

[🧵 conversation](https://history.futureofcoding.org/history/weekly/2023/08/W2/linking-together.html#2023-08-11T18:57:11.038Z)

Has anyone read [Who Can Name the Bigger Number?](https://www.scottaaronson.com/writings/bignumbers.html) It was just recommended to me at work

💡 [Live, Rich, and Composable: Qualities for Programming Beyond Static Text](https://arxiv.org/pdf/2303.06777.pdf) via [Lu Wilson](https://twitter.com/TodePond)

[🧵 conversation](https://history.futureofcoding.org/history/weekly/2023/08/W2/linking-together.html#2023-08-12T08:31:31.772Z)

This was posted here back in March but it didn't get much discussion (at least, that's what the archive says), so I'm reposting: [arxiv.org/pdf/2303.06777.pdf](https://arxiv.org/pdf/2303.06777.pdf)


📝 [Do Machine Learning Models Memorize or Generalize?](https://pair.withgoogle.com/explorables/grokking) via [Kartik Agaram](http://akkartik.name/about)

[🧵 conversation](https://history.futureofcoding.org/history/weekly/2023/08/W2/of-ai.html#2023-08-10T22:15:09.739Z)

 *Grokking*






> In 2021, researchers made a striking discovery while training a series of tiny models on toy tasks . They found a set of models that suddenly flipped from memorizing their training data to correctly generalizing on unseen inputs after training for much longer. This phenomenon – where generalization seems to happen abruptly and long after fitting the training data – is called
>  _grokking_
>  and has sparked a flurry of interest .

>An interactive introduction to grokking and mechanistic interpretability.

