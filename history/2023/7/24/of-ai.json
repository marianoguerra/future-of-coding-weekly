[
    {
        "client_msg_id": "8b783740-38e0-4805-b039-8ad78126615e",
        "type": "message",
        "text": "Is anybody using gpt to autogenerate blog posts or other educational media?",
        "user": "UL5AX4G2H",
        "ts": "1690234610.665789",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "kV=uC",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Is anybody using gpt to autogenerate blog posts or other educational media?"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "thread_ts": "1690234610.665789",
        "reply_count": 4,
        "reply_users_count": 3,
        "latest_reply": "1690276759.176049",
        "reply_users": [
            "UL5AX4G2H",
            "UGWUJUZHT",
            "U035QJ14NN9"
        ],
        "is_locked": false,
        "subscribed": false
    },
    {
        "client_msg_id": "bfc83bb0-1e99-4a30-b3e7-b9e541a0e906",
        "type": "message",
        "text": "I've been messing around with generating simple blog posts in substack, so want to see what others are doing that's like this",
        "user": "UL5AX4G2H",
        "ts": "1690234656.775569",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "7EAtC",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I've been messing around with generating simple blog posts in substack, so want to see what others are doing that's like this"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "thread_ts": "1690234610.665789",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "2a1c8f53-36d5-4b99-b1b5-8da7edd85e18",
        "type": "message",
        "text": "FYI - I\u2019m thinking on how to sketch out ideas in a \u201cmind map\u201d (Kinopio) and have GPT ghost-write a fleshed out post from that (this would also be good for README.md files in dev repos).\n\nI had some success with jamming Kinopio-generated JSON into KAGI Summarizer, but hit a road-block when I tried to pour the same JSON into ChatGPT-3 (prompt too long, and, won\u2019t accept URLs).\n\nI\u2019m thinking of trying again with Llama-2, but, thus far haven\u2019t gone down that learning curve...\n\nI, also, have had success with writing a terse paragraph, then having ChatGPT turn it into a chapter for a \u201cbook\u201d. Prompt: \u2018edit this chapter and flesh it out \u2019 &lt;followed by the terse paragraph text&gt;), result: successfully generated chapter(s), but, lots of reading for me.  Approving, editing, etc. - stuff I don\u2019t really want to be bothered with. I will gladly share, but, it\u2019s too long to post here.\n\nIt looks like feeding short articles / chapters to ChatGPT will work, but I wish for something even simpler.   I guess I could dump my thoughts into Kinopio, then dictate chapters into text files while prompting myself by looking at the Kinopio thoughts and recording my ramblings (say, using speech-to-text).  If my other experiments fail, I will fall back to this strategy.\n\nAlong the way, someone mentioned \u2018<http://type.ai|type.ai>\u2019 to me.  It looks interesting but I haven\u2019t had time to check it out.\n\nAn experiment with <http://type.ai|type.ai> managed to produce a reasonable summary of the same Kinopio thought-map as above, but, the experimenter typed the text for each thought-bubble in manually.\n\nI would enjoy hearing/reading about any experiments you try.  In fact, if you are successful, you can have GPT generate a blog post about how you did it :-).",
        "user": "UGWUJUZHT",
        "ts": "1690254776.509349",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "53E",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "FYI - I\u2019m thinking on how to sketch out ideas in a \u201cmind map\u201d (Kinopio) and have GPT ghost-write a fleshed out post from that (this would also be good for README.md files in dev repos).\n\nI had some success with jamming Kinopio-generated JSON into KAGI Summarizer, but hit a road-block when I tried to pour the same JSON into ChatGPT-3 (prompt too long, and, won\u2019t accept URLs).\n\nI\u2019m thinking of trying again with Llama-2, but, thus far haven\u2019t gone down that learning curve...\n\nI, also, have had success with writing a terse paragraph, then having ChatGPT turn it into a chapter for a \u201cbook\u201d. Prompt: \u2018edit this chapter and flesh it out \u2019 <followed by the terse paragraph text>), result: successfully generated chapter(s), but, lots of reading for me.  Approving, editing, etc. - stuff I don\u2019t really want to be bothered with. I will gladly share, but, it\u2019s too long to post here.\n\nIt looks like feeding short articles / chapters to ChatGPT will work, but I wish for something even simpler.   I guess I could dump my thoughts into Kinopio, then dictate chapters into text files while prompting myself by looking at the Kinopio thoughts and recording my ramblings (say, using speech-to-text).  If my other experiments fail, I will fall back to this strategy.\n\nAlong the way, someone mentioned \u2018"
                            },
                            {
                                "type": "link",
                                "url": "http://type.ai",
                                "text": "type.ai"
                            },
                            {
                                "type": "text",
                                "text": "\u2019 to me.  It looks interesting but I haven\u2019t had time to check it out.\n\nAn experiment with "
                            },
                            {
                                "type": "link",
                                "url": "http://type.ai",
                                "text": "type.ai"
                            },
                            {
                                "type": "text",
                                "text": " managed to produce a reasonable summary of the same Kinopio thought-map as above, but, the experimenter typed the text for each thought-bubble in manually.\n\nI would enjoy hearing/reading about any experiments you try.  In fact, if you are successful, you can have GPT generate a blog post about how you did it :-)."
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "thread_ts": "1690234610.665789",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "A634A436-6CB5-4E18-8599-AED8435E6FE5",
        "type": "message",
        "text": "Why should we wish to inflate our writing with AI?",
        "user": "U035QJ14NN9",
        "ts": "1690273035.410009",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Mz1G",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Why should we wish to inflate our writing with AI?"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "thread_ts": "1690234610.665789",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "f88b248e-89a7-4e89-ba4d-427a666c288b",
        "type": "message",
        "text": "IMO, it comes down to a projectional-editing / syntax-is-cheap argument.\n\nExperts prefer to compress information, novices need to be reminded of what each part means.\n\nFor example, the equation of a line can be expressed in expert notation as:\n\ny = mx + b\n\nwhereas, novices (e.g. school children) might want to see:\n\ny = slope times x + y_intercept\n\nThe underlying idea is the same, just the expression (syntax) changes.\n\nAnother example: IMO, Common Lisp has one-of-everything.  Why not express ALL programs in CL syntax, RPN?  Why do we insist on using other syntaxes, like Python or Haskell? (N.B. CL already has a syntax for type information, also RPN).\n\nAnother example: why not use even-more-concise binary lambda calculus notation for everything?  Quick, what does this program do? `\u03bbab.b(\u03bbcde.c(\u03bbfg.g(fd))(\u03bbf.e)(\u03bbf.f))a` (<https://justine.lol/lambda/>)\n\nOr, why do we bother with opcodes?  Why not just use transistors?\n\nThis question brings up an interesting issue.  Is written language an innately expressive form, or just something that we\u2019ve learned to live with, biased by our available media (e.g. clay tablets, paper, graphite, rubber)?  I find myself wanting to watch a YouTube rather than reading a paper...",
        "user": "UGWUJUZHT",
        "ts": "1690276759.176049",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "xFMBR",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "IMO, it comes down to a projectional-editing / syntax-is-cheap argument.\n\nExperts prefer to compress information, novices need to be reminded of what each part means.\n\nFor example, the equation of a line can be expressed in expert notation as:\n\ny = mx + b\n\nwhereas, novices (e.g. school children) might want to see:\n\ny = slope times x + y_intercept\n\nThe underlying idea is the same, just the expression (syntax) changes.\n\nAnother example: IMO, Common Lisp has one-of-everything.  Why not express ALL programs in CL syntax, RPN?  Why do we insist on using other syntaxes, like Python or Haskell? (N.B. CL already has a syntax for type information, also RPN).\n\nAnother example: why not use even-more-concise binary lambda calculus notation for everything?  Quick, what does this program do? "
                            },
                            {
                                "type": "text",
                                "text": "\u03bbab.b(\u03bbcde.c(\u03bbfg.g(fd))(\u03bbf.e)(\u03bbf.f))a",
                                "style": {
                                    "code": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " ("
                            },
                            {
                                "type": "link",
                                "url": "https://justine.lol/lambda/"
                            },
                            {
                                "type": "text",
                                "text": ")\n\nOr, why do we bother with opcodes?  Why not just use transistors?\n\nThis question brings up an interesting issue.  Is written language an innately expressive form, or just something that we\u2019ve learned to live with, biased by our available media (e.g. clay tablets, paper, graphite, rubber)?  I find myself wanting to watch a YouTube rather than reading a paper..."
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "attachments": [
            {
                "from_url": "https://justine.lol/lambda/",
                "service_icon": "https://justine.lol/apple-touch-icon.png",
                "id": 1,
                "original_url": "https://justine.lol/lambda/",
                "fallback": "Lambda Calculus in 383 Bytes",
                "text": "Programming language with a single keyword.",
                "title": "Lambda Calculus in 383 Bytes",
                "title_link": "https://justine.lol/lambda/",
                "service_name": "justine.lol"
            }
        ],
        "thread_ts": "1690234610.665789",
        "parent_user_id": "UL5AX4G2H",
        "reactions": [
            {
                "name": "exploding_head",
                "users": [
                    "U02NU8FTL5N"
                ],
                "count": 1
            }
        ]
    }
]