[
    {
        "client_msg_id": "e9ddb0aa-255d-484b-b2e0-5407447d39fc",
        "type": "message",
        "text": "I don't know if I want to laugh of cry :joy:. This is done just by duct-taping a bunch of APIs and GPT-4 prompts together. Maybe the future of programming will be more English than code? <https://chameleon-llm.github.io/>. Paper (<https://arxiv.org/abs/2304.09842>) and Code (<https://github.com/lupantech/chameleon-llm>)",
        "user": "U01L6HZEHFX",
        "ts": "1681986431.736239",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "pcG27",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I don't know if I want to laugh of cry "
                            },
                            {
                                "type": "emoji",
                                "name": "joy",
                                "unicode": "1f602"
                            },
                            {
                                "type": "text",
                                "text": ". This is done just by duct-taping a bunch of APIs and GPT-4 prompts together. Maybe the future of programming will be more English than code? "
                            },
                            {
                                "type": "link",
                                "url": "https://chameleon-llm.github.io/"
                            },
                            {
                                "type": "text",
                                "text": ". Paper ("
                            },
                            {
                                "type": "link",
                                "url": "https://arxiv.org/abs/2304.09842"
                            },
                            {
                                "type": "text",
                                "text": ") and Code ("
                            },
                            {
                                "type": "link",
                                "url": "https://github.com/lupantech/chameleon-llm"
                            },
                            {
                                "type": "text",
                                "text": ")"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "edited": {
            "user": "U01L6HZEHFX",
            "ts": "1681986471.000000"
        },
        "attachments": [
            {
                "from_url": "https://arxiv.org/abs/2304.09842",
                "service_icon": "https://static.arxiv.org/static/browse/0.3.4/images/icons/apple-touch-icon.png",
                "thumb_url": "https://static.arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png",
                "thumb_width": 1200,
                "thumb_height": 700,
                "id": 1,
                "original_url": "https://arxiv.org/abs/2304.09842",
                "fallback": "arXiv.org: Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
                "text": "Large language models (LLMs) have achieved remarkable progress in various\nnatural language processing tasks with emergent abilities. However, they face\ninherent limitations, such as an inability to access up-to-date information,\nutilize external tools, or perform precise mathematical reasoning. In this\npaper, we introduce Chameleon, a plug-and-play compositional reasoning\nframework that augments LLMs to help address these challenges. Chameleon\nsynthesizes programs to compose various tools, including LLM models,\noff-the-shelf vision models, web search engines, Python functions, and\nrule-based modules tailored to user interests. Built on top of an LLM as a\nnatural language planner, Chameleon infers the appropriate sequence of tools to\ncompose and execute in order to generate a final response. We showcase the\nadaptability and effectiveness of Chameleon on two tasks: ScienceQA and TabMWP.\nNotably, Chameleon with GPT-4 achieves an 86.54% accuracy on ScienceQA,\nsignificantly improving upon the best published few-shot model by 11.37%; using\nGPT-4 as the underlying LLM, Chameleon achieves a 17.8% increase over the\nstate-of-the-art model, leading to a 98.78% overall accuracy on TabMWP. Further\nstudies suggest that using GPT-4 as a planner exhibits more consistent and\nrational tool selection and is able to infer potential constraints given the\ninstructions, compared to other LLMs like ChatGPT.",
                "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
                "title_link": "https://arxiv.org/abs/2304.09842",
                "service_name": "arXiv.org"
            }
        ]
    }
]