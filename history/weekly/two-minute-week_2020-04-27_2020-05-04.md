*[2020-04-20 07:27:05]* **Unknown User**:

MSG NOT FOUND


> *[2020-04-26 22:03:34]* **Ivan Reese**:

Gary Bernhardt's "The Birth & Death of JavaScript" continues to come true. Love it.

---

*[2020-04-26 13:51:49]* **Unknown User**:

MSG NOT FOUND


> *[2020-04-26 22:13:01]* **Ivan Reese**:

This is beautiful and elegant. So inspiring to see. I wish I could say more than that, but what you've shown seems very well-considered and I don't have anything meaningful to add.

I also really appreciate the effort you're putting into these videosÂ â€” the planning and editing are a real benefit. If there was a way to adjust how you're processing the audio so it doesn't cut in and out so abruptly, that'd help me focus more on the substance of what you're saying. I'd probably find it less distracting if you didn't filter/cut out the "silences" (room tone) between your sentences at all.


> *[2020-04-27 07:23:04]* **Yair Chuchem**:

**Ivan Reese** true. Iâ€™m actually quite embarrased about the sound. for our youtube videos I actually went and recorded the sound with a proper setup but here itâ€™s been a very quick thing..


> *[2020-04-27 07:35:06]* **Ivan Reese**:

Quick is good! All I mean to say is â€”Â the background noise is not bad. Embrace it, for this format. I'm also going to be embracing a "quick and dirty" aesthetic for my videos. Your audio has good levels, and your voice is loud and clear and pleasing to listen to. No need to try to carve out silences.


> *[2020-04-27 07:38:05]* **Yair Chuchem**:

Itâ€™s not really a conscious effort to carve out, but more of an edit of different takes Iâ€™m recording within final cut as I make the edit, where final cut doesnâ€™t make it easy if you want to cross fade etc as by default itâ€™s just layed out as layers attached to the magnetic timeline, but yeah I can try to group together the clips. wrt carving silences actually in our youtube videos my audio engineer friend did put a noise gate on them but the recordings were much better to begin with and didnâ€™t have much noticeable noise..


> *[2020-04-27 14:11:39]* **Sol Bekic**:

**Yair Chuchem** i recorded my video with a pretty bad setup (webcam) and did some very basic noise removal in Audacity - its very easy to do: <https://www.youtube.com/watch?v=xkpzHJGE4Dk>
That and fade in/out should take care of most of it I think :)


> *[2020-04-27 14:16:14]* **Yair Chuchem**:

**Sol Bekic** thanks! I also recorded with a webcam and I do have access and some knowledge of audio processing tools, I develop them for a living.. Itâ€™s just that I didnâ€™t bother â€œgoing proâ€ for a quick 2 minutes video. For Lamduâ€™s videos on youtube Iâ€™ve explicitly went to record at the pro recording studio at my office (which at the moment I donâ€™t have access to while quarantining at home) and recorded in a quality room with quality hardware and had a pro audio engineer process it a bit (Iâ€™m just a programmer after all..).
For here I just wanted something ready without much effort and recorded with my webcam and only applied the untouched compressor preset I had working on my last youtube video.. But I get it, and will make an effort to improve the sound editing for next weekâ€™s video ðŸ™‚


> *[2020-04-27 14:18:44]* **Sol Bekic**:

**Yair Chuchem** oh, I see. In any case, I'm really liking your to-the-point videos! You definitely have my attention ;)


> *[2020-04-27 18:51:24]* **Ivan Reese**:

The best example of the problem I have is, say, from 0:10 to 0:20 in the video. The room noise in the background is less distracting than the fact that it keeps cutting in and out. So when editing in Final Cut, you can the "empty" room tone between your sentences, rather than cutting your audio clips so that they only exist when you're speaking. Again, this isn't super important, it's just a small thing that'd hopefully not cost you any additional effort but make the videos easier to follow.

As for Sol's suggestion to use noise removal, I find that that always introduces artifacts, and the artifacts are just as distracting as the noise. Better to just live with the noise floor, I say â€” but that's just my personal taste.


> *[2020-04-27 19:06:14]* **Yair Chuchem**:

**Ivan Reese** I know that some very serious folks swear by some noise removal tools like Izotopeâ€™s, and also Accusonusâ€™s not bad I think


> *[2020-04-27 23:39:57]* **Ivan Reese**:

Yeah, Izotope makes some amazing tools. But a lot of your success with noise removal depends on the kind of noise being removed. Is it hiss? Is it a pure tone, like 60hz line hum? Those are easier to remove without side effects. But lots of noises are less easily characterized, and even the best tools I've seen will have trouble with them.

That said.. I've yet to see anyone apply ML to this problem, and it seems like the sort of thing that'd be _very_ well suited. Could be there are better solutions now since the last time I checked, even.

---

*[2020-04-26 19:08:35]* **Unknown User**:

MSG NOT FOUND


> *[2020-04-26 22:17:37]* **Ivan Reese**:

Can we nerd-out for a sec on that FFT view? I love the per-channel idea. Was that inspired by anything in particular?


> *[2020-04-26 22:19:26]* **Ivan Reese**:

(My favourite audio tool, perhaps my favourite software tool of all, is the EQ8 in Ableton Live, which is a parametric EQ with an excellent GUI and a fairly nice spectrum behind it. So it always excites me to see people doing cool things with EQ GUIs or FFT displays)


> *[2020-04-26 22:53:46]* **Chris Maughan**:

Because I need visualisations to pretty much understand anything hard, I needed the FFT to help me understand the synth and check for problems (it has already shown an issue with my triangle wave tables). I solved the problem of polyphony by making my data flow pins have multiple channels - so effectively every unit will generate data for all input channels. That meant that the multiple visualisations just fell out of the design (but note that the visualiser node is attached into the graph before a â€˜flatteningâ€™ node which combines the polyphony into a single channel (before widening it back to stereo!) hope that makes sense **Ivan Reese** !


> *[2020-04-27 08:54:48]* **Ivan Reese**:

That's an incredibly cool idea. If I understand it right, it's as though all your nodes are doing multiple channels in parallel? That feels a bit like SIMD for visual programming. I'm going to have to think about this a lot more. I think this has very interesting ramifications.


> *[2020-04-27 11:38:50]* **Chris Maughan**:

Yes, that's right.  Imagine the user is playing C Major; 3 notes.  Those 3 notes go into the Oscillator node as control signals.  The oscillator then outputs 3 arrays, each containing, say, 512 audio samples for each of the 3 notes.  The 'output' pin of the oscillator now has 3 internal data streams.  So each unit can operate on the data as a whole.  The next graph node can understand this data and do the right thing.  Imagine that you have 2 oscillators tied into the same note input.  Now you have 2 units emitting the same 3 channels.  If you want to combine the oscillators for each node, then the mixer has no trouble distinguishing the pairs of streams to combine, because each note channel has been tagged with the Id of the note.
```m_spOutData->MatchChannelInput(*pFlowData);
auto& outChannels = m_spOutData->GetChannels();
auto& inChannels = pFlowData->GetChannels();

for (uint32_t ch = 0; ch < pFlowData->GetNumChannels(); ch++)
{
    for (uint32_t i = 0; i < maud.genFrameCount; i++)
    {
        sp_moogladder_compute(maud.pSP, m_vecMoogLadder[ch], &inChannels[ch].data[i], &outChannels[ch].data[i]);
     }
}```
Above is the simple example of a Low Pass Filter.  It first reads in the input channels, and in this case ensures that the output channels match, before applying the effect.


> *[2020-04-27 11:41:36]* **Chris Maughan**:

I doubt this is a new approach; but it seemed the most sensible to me.  I thought about potentially splitting the directed graph into separate instances for each note, but that felt like it was going to get complicated really quickly.


> *[2020-04-27 11:44:39]* **Chris Maughan**:

I also considered threading issues, but this actually works really well from that point of view.  In my performance analysis, i've noticed that most of the work is in the wave table sampling.  Since all those units are just sitting generating arrays of data for all the held down notes, they can effectively be run in parallel if necessary


> *[2020-04-27 11:45:22]* **Chris Maughan**:




> *[2020-04-27 11:49:03]* **Chris Maughan**:

Above is a profile of the graph I showed with several notes playing.  The blocks on the right are the FFT for each note, in worker threads.  The block on the left are the oscillators.  Each oscillator is generating all samples for all notes, but you can see that they run sequentially since they are on the same audio thread.  I'll probably move them to workers (it isn't hard), but I don't want to optimise up front until I have a better feel for things. Since premature optimization is the root of all evil, as you know ðŸ˜‰

---

*[2020-04-26 21:21:34]* **Unknown User**:

MSG NOT FOUND


> *[2020-04-26 22:22:35]* **Ivan Reese**:

Nice! If you're wondering what to do a video about next week, I'd love to see a 2-minute demo of the layout system. It's the sort of thing that I've heard you talk about, but it's hard to express what makes it interesting through words alone.


> *[2020-04-27 07:51:57]* **Mariano Guerra**:

looks like the trailer of a netflix show, now I want to see more ðŸ˜„


> *[2020-04-27 09:28:14]* **Duncan Cragg**:

Yeah "renaissance proportions" needs elaboration!


> *[2020-04-28 07:58:36]* **William Taysom**:

I'll second renaissance proportions.  Likewise it is good to say, "with Beads we're focusing on this and wanting to ignore that."  It's a great way to describe any project.

---

*[2020-04-27 09:00:41]* **Ivan Reese**:

I just want to take a moment and say â€” this channel is making me incredibly happy. I have been absolutely delighted by every one of these videos, and I'm feeling so inspired by all the work you're doing. There are already a lot of fascinating ideas on display here, and the two minute format is so digestible that it's cracking an accessibility barrier that's kept me from properly appreciating many of them before. Thank you for making these videos and fostering the discussions, and I hope we'll be able to keep the momentum going.

---

*[2020-04-29 18:26:48]* **Dan Swirsky**:

Here's Hilltop's first two-minute (and change) video!

---

*[2020-04-29 18:27:27]* **Dan Swirsky**:

[Hilltop - Episode 1](https://youtu.be/Z8nXFLWkQmw)


> *[2020-04-29 18:34:47]* **Ivan Reese**:

The video is set to "private" â€” you probably want to set it as "unlisted"


> *[2020-04-29 18:37:44]* **Dan Swirsky**:

Done


> *[2020-04-29 20:00:42]* **Edward de Jong**:

Its nice to see a proportional font programming system. Fixed pitch is so archaic, however many fonts have too thin punctuation marks so unless you use a custom font it is easier to work with. Not sure that notating literals by using a darker color is going to pan out. Sooner or later you are going to want to use that annotation for some other purpose, and are quotes really so bad?


> *[2020-04-30 03:22:10]* **William Taysom**:

Getting Self vibes.  Man, I remember when Verdana was my go to programming font.  ðŸº


> *[2020-04-30 16:04:09]* **Ivan Reese**:

Is there a particular kind of app that you intend for Hilltop to be well suited to? Something like hypertext, or CRUD? Also, curious to hear more about what direction your exploration is headed.


> *[2020-04-30 18:06:56]* **Dan Swirsky**:

I don't have a type of app in mind. I'm exploring several things at the same time:
â€¢ What if we took away the row-and-column aspect of a spreadsheet, and just kept the cells, where a cell is something that holds an object definition?
â€¢ And what if when you decide to create a new cell/object definition, itâ€™s based on a predefined object type, such as of a number, text, an image, a sound, etc. (as opposed to spreadsheets, where you can't immediately tell whether a number in a cell is numeric or text)?
â€¢ And what if predefined object types have predefined attributes, such as â€˜valueâ€™ for primitive object types, X and Y coordinates for object types that can be displayed, etc.?
â€¢ And what if the programmer provides value definitions to whichever attributes she likes, such as literal values or formulas that reference values of other attributes?
â€¢ And what if an app is defined as a collection of these cells/object definitions and the value definitions of their attributes, just as in a spreadsheet?
â€¢ And what if "running" the app simply means instantiating the cells and letting them "do" whatever their object types "do" (e.g., an image-type object "displays" if its X and Y attributes have values; a sound-type object makes a sound if its â€˜volumeâ€™ attribute is set to some nonzero value which, according to its value definition, happens when the value of some boolean-type attribute of another object changes)?
â€¢ And what if, as in a spreadsheet, a behind-the-scenes manager propagates value changes to various object attributes when the values of other object attributes change, and this is what manages the appâ€™s behavior?
â€¢ And what would all this look like as text-based programming (as opposed to visual programming like Scratch)?
â€¢ And could we eliminate syntax errors using a structure editor, where the keyboard is only used when naming objects, entering literal values, and editing comments?
â€¢ And what if the structure editor is in the form of context menus?
â€¢ And what if tapping different types of code elements opens different context menus, each only showing valid programming options based on the tapped code element type?
â€¢ And would this mean that we could program on our phones and tablets?


> *[2020-04-30 18:08:37]* **Ivan Reese**:

This feels like a rich vein! I'm excited to see where you go with it. Do you have a working prototype? How's that going?


> *[2020-04-30 18:18:13]* **Dan Swirsky**:

Nope. It's design and mockups only, at least until I find my Wozniak


> *[2020-04-30 19:05:14]* **Ivan Reese**:

Put out the call here! According to the survey, there are a lot of folks who are looking for projects to contribute to.


> *[2020-05-03 01:07:14]* **Garth Goldwater**:

this is absolutely great and if you havenâ€™t yet you should absolutely check out self **Dan Swirsky** [https://youtu.be/3ka4KY7TMTU](https://youtu.be/3ka4KY7TMTU)


> *[2020-05-03 01:07:45]* **Garth Goldwater**:

[https://youtu.be/Ox5P7QyL774](https://youtu.be/Ox5P7QyL774) itâ€™s pretty much dead now but a huge inspiration 


> *[2020-05-03 07:18:50]* **Dan Swirsky**:

Yup. Saw these a while back.


> *[2020-05-03 08:51:09]* **William Taysom**:

A classic.  ðŸ˜‰

---

*[2020-05-02 11:27:43]* **Mariano Guerra**:

Instadeq Week in Two Minutes #3: Generalized IO UI & history, Selected output on charts, Keep UI state on IO update, Remove cycles in sankey chart,  Providers and more <https://youtu.be/WcaY3KLj7pY>


> *[2020-05-02 14:17:12]* **Yair Chuchem**:

The system looks really cool! But I have a comment about the video.
You do have a lot to show, much more than would fit into a 2 minute video each time, which is impressive! And you choose to pack a lot in by talking very fast. I think that it would be better to choose what to include in the video and what not, talk in a slower pace while leaving some things out.
It may just be my personal perspective so anyone who feels the pace is actually fine, feel free to put a thumb down on this comment and I wonâ€™t be offended ðŸ™‚ Just so that Mariano could tell if this is indeed common among viewers or not.


> *[2020-05-02 14:25:11]* **Mariano Guerra**:

I get the "you talk very fast" feedback a lot ðŸ™‚ I think this week will be the last where there's too much to "show" in two minutes since I will be moving to the backend and polishing, so I will be forced to show less because there will be less that can be shown ðŸ˜„


> *[2020-05-02 14:26:29]* **Mariano Guerra**:

it's my fault, since I don't know how to edit video and I use this video to keep different groups of people up to date (instead of doing one for each) I pack everything in in one shot without intermissions, I should learn to edit video ðŸ˜•


> *[2020-05-02 14:30:14]* **Yair Chuchem**:

Itâ€™s not so hard. I donâ€™t recognise your WM so you are probably not on macOS or Windows (which ship with decent video editors). In that case there are several open-source editors for Linux. I also heard of one thatâ€™s specifically intended for editing screen casts - <https://owickstrom.github.io/komposition/>


> *[2020-05-02 14:35:15]* **Mariano Guerra**:

I tried and I can do a decent job at it, but what happened many times is that I lost quality on the transcoding and then later youtube lost more quality even, when I upload the raw screencast I don't loose any quality. I guess it's some combination of intermediate formats, parameters and resolutions, that I have to find the right combination of


> *[2020-05-03 01:03:25]* **Garth Goldwater**:

youtube butchers everyoneâ€™s videosâ€”thereâ€™s not much to be done. i would love to see a walkthrough of instadeq that starts with a single dataset and a series of questions a user might have and then answers them one by one (including the import stuff, etc) in little 2 minute increments. like a series of skits about someone getting a lot of good work out of it


> *[2020-05-03 01:04:15]* **Garth Goldwater**:

that said, i like your videos as they are and am subscribed to you on youtube. iâ€™m impressed by your output speed and youâ€™re already in the top 5% of software demos on youtube no problem


> *[2020-05-03 11:43:37]* **Mariano Guerra**:

I will do the walk trough series once the ui stabilizes, so I don't have to record them again when the ui changes to avoid confusing viewers following them :)

---

*[2020-05-03 19:08:27]* **Chris Maughan**:

Third update for my Live Coding project - still focusing on the sythesizer creation.


> *[2020-05-03 19:09:12]* **Chris Maughan**:

I'm not improving my ability to speak over this stuff, despite trying to make a script!  I hope it is clear enough!


> *[2020-05-03 19:37:30]* **Mariano Guerra**:

how do I know what is connected to what? top left to bottom right?


> *[2020-05-03 19:37:57]* **Mariano Guerra**:

or the relationships make sense for people that know about synths?