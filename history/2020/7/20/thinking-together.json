[
    {
        "client_msg_id": "bf8940fd-2522-421a-ae42-6e1c30336e81",
        "type": "message",
        "text": "I’ve found this library useful in the past, especially for quick parsing tasks. <https:\/\/github.com\/orangeduck\/mpc>\nLast time I did a language, I rolled my own tokenizer and lexer.  I find things easier to understand that way.  I know that Jonathan Blow’s Jai language uses the same hand rolled approach.",
        "user": "UUQ2EQW21",
        "ts": "1595228708.034300",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "g2266cacc8f3",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/2266cacc8f3c9964e7bfb1c357bf6873.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0024-72.png",
            "first_name": "Chris",
            "real_name": "Chris Maughan",
            "display_name": "Chris Maughan",
            "team": "T5TCAFTA9",
            "name": "mornymorny",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "attachments": [
            {
                "fallback": "orangeduck\/mpc",
                "text": "A Parser Combinator library for C",
                "title": "orangeduck\/mpc",
                "footer": "<https:\/\/github.com\/orangeduck\/mpc|orangeduck\/mpc>",
                "id": 1,
                "footer_icon": "https:\/\/github.githubassets.com\/favicon.ico",
                "ts": 1380201606,
                "color": "24292f",
                "fields": [
                    {
                        "title": "Stars",
                        "value": "1923",
                        "short": true
                    },
                    {
                        "title": "Language",
                        "value": "C",
                        "short": true
                    }
                ],
                "mrkdwn_in": [
                    "text",
                    "fields"
                ],
                "bot_id": "B011KHY4N3Y",
                "app_unfurl_url": "https:\/\/github.com\/orangeduck\/mpc",
                "is_app_unfurl": true
            }
        ],
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "7H1A",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I’ve found this library useful in the past, especially for quick parsing tasks. "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/github.com\/orangeduck\/mpc"
                            },
                            {
                                "type": "text",
                                "text": "\nLast time I did a language, I rolled my own tokenizer and lexer.  I find things easier to understand that way.  I know that Jonathan Blow’s Jai language uses the same hand rolled approach."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595219570.025900",
        "parent_user_id": "U010328JA1E"
    },
    {
        "client_msg_id": "d302e963-aea6-4f2e-a2af-3d2c46f5e118",
        "type": "message",
        "text": "It has the nice feature of being able to just pass a grammar to the library, and it builds the parser combinators.  Very quick and easy.",
        "user": "UUQ2EQW21",
        "ts": "1595229509.034600",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "g2266cacc8f3",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/2266cacc8f3c9964e7bfb1c357bf6873.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0024-72.png",
            "first_name": "Chris",
            "real_name": "Chris Maughan",
            "display_name": "Chris Maughan",
            "team": "T5TCAFTA9",
            "name": "mornymorny",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "16J",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "It has the nice feature of being able to just pass a grammar to the library, and it builds the parser combinators.  Very quick and easy."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595219570.025900",
        "parent_user_id": "U010328JA1E"
    },
    {
        "client_msg_id": "12a571b9-0010-4683-a4bf-346678ee0a94",
        "type": "message",
        "text": "I find the question a bit underspecified, as which tools one might choose will hinge on both the specifics of what one is building and a variety of personal preferences. In my case, I like languages\/environments that allow me to develop new semantics from within them, only worrying about adding syntactic affordances later. If one prefers that approach, it's hard to beat <https:\/\/racket-lang.org>",
        "user": "U013ZLJARC7",
        "ts": "1595230411.034800",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gf4ae9e5b293",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/f4ae9e5b29386489b18b3bc6b1f41a22.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0024-72.png",
            "first_name": "",
            "real_name": "Jack Rusher",
            "display_name": "Jack Rusher",
            "team": "T5TCAFTA9",
            "name": "jack529",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "CLlN",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I find the question a bit underspecified, as which tools one might choose will hinge on both the specifics of what one is building and a variety of personal preferences. In my case, I like languages\/environments that allow me to develop new semantics from within them, only worrying about adding syntactic affordances later. If one prefers that approach, it's hard to beat "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/racket-lang.org"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595219570.025900",
        "parent_user_id": "U010328JA1E",
        "reactions": [
            {
                "name": "heavy_plus_sign",
                "users": [
                    "UP28ETUSE",
                    "UJBAJNFLK"
                ],
                "count": 2
            },
            {
                "name": "heavy_check_mark",
                "users": [
                    "U010328JA1E"
                ],
                "count": 1
            }
        ]
    },
    {
        "client_msg_id": "3dae2973-aaaf-43e2-88b2-e217e74fe769",
        "type": "message",
        "text": "I don't know exactly how Pulimi works, but I have some experience with Terraform (I use it on my last project as a corporate employee).\n\nThe point is that generally you define a desired state and ask Terraform to do all it can to create\/modify the corresponding infrastructure. And for that it analyzes your code, checks that everything is OK and proposes you a \"plan\" which describes what Terraform will actually do: create new resource, deleting some others or just modifying some. Terraform then asks you if you want to proceed and then actually do the job. Of course, you can tell to Terraform to apply changes automatically without asking, if you are very confident about you code or don't care to break things, but generally this is desired thing to be asked because of the tricky nature of infrastructure management.\n\nSo I guess the scenario \"create something, wait a bit, create something else\" is not very common and is actually \"not in the spirit\". But if this use case is really needed, if I recall well there are some trick to allow doing this kind of pause. In the definition of the first resource you add a script that sleep 60 seconds, maybe with some monitoring, and make the second resource depends on this resource, which will make Terraform wait the end of the script before creating the second resource. The paradigm is to define resources and dependencies.\n\nFor loops, there is something that allows you to create a set of ressources with a single definition. I used it to generate thousands of VM. In fact you have a series of tools for \"loop\/conditionals\" at the language level, but always in a spirit of \"templating\".\n\nBut after a bit of practicing, I felt sometime that I would have prefer to code things with a general purpose language. For things like getting env vars, managing some files, etc. I guess it is what Pulumi is about, and I am very curious about it.",
        "user": "UJ6LDMMN0",
        "ts": "1595232493.035700",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "0c210ee2df74",
            "image_72": "https:\/\/avatars.slack-edge.com\/2021-11-13\/2722434855730_0c210ee2df74838f8683_72.png",
            "first_name": "nicolas",
            "real_name": "nicolas decoster",
            "display_name": "ogadaki",
            "team": "T5TCAFTA9",
            "name": "nicolas.decoster",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "1Uot",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I don't know exactly how Pulimi works, but I have some experience with Terraform (I use it on my last project as a corporate employee).\n\nThe point is that generally you define a desired state and ask Terraform to do all it can to create\/modify the corresponding infrastructure. And for that it analyzes your code, checks that everything is OK and proposes you a \"plan\" which describes what Terraform will actually do: create new resource, deleting some others or just modifying some. Terraform then asks you if you want to proceed and then actually do the job. Of course, you can tell to Terraform to apply changes automatically without asking, if you are very confident about you code or don't care to break things, but generally this is desired thing to be asked because of the tricky nature of infrastructure management.\n\nSo I guess the scenario \"create something, wait a bit, create something else\" is not very common and is actually \"not in the spirit\". But if this use case is really needed, if I recall well there are some trick to allow doing this kind of pause. In the definition of the first resource you add a script that sleep 60 seconds, maybe with some monitoring, and make the second resource depends on this resource, which will make Terraform wait the end of the script before creating the second resource. The paradigm is to define resources and dependencies.\n\nFor loops, there is something that allows you to create a set of ressources with a single definition. I used it to generate thousands of VM. In fact you have a series of tools for \"loop\/conditionals\" at the language level, but always in a spirit of \"templating\".\n\nBut after a bit of practicing, I felt sometime that I would have prefer to code things with a general purpose language. For things like getting env vars, managing some files, etc. I guess it is what Pulumi is about, and I am very curious about it."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595084421.458800",
        "parent_user_id": "UKP3B2J5D"
    },
    {
        "client_msg_id": "9f778d77-3555-48a3-b326-bb88d75b6614",
        "type": "message",
        "text": "Pulumi is very much like Terraform, it actually uses the Terraform providers behind the scenes (Pulumi does not generate files, as AWS CDK does). The only major difference is that you declare that desired state using an API of a general purpose language. But otherwise, Pulumi does exactly what <@UJ6LDMMN0> said Terraform does — keeping that desired state, creating a plan, previewing it, applying it, etc.\n\nRegarding <@UCUSW7WVD>'s problem, I think the solution in Pulumi would be similar to the one in Terrraform. Define a custom resource that does nothing than just block execution for an amount of time and then to the health check, then use this resource as a dependency of the second host you want to create.\n\nGoing back to the original question of whether it would make sense to have a visual editor for k8s config files, I don't think it would help that much. The problem with YAML is not that it's YAML, it's that it offers no means composition and abstraction. With only a visual editor that all it does is to offer affordances for YAML concepts, things will be the same. But, if that visual editors starts adding some PL concepts, such as variables and functions, then it can be useful, but at this point I think it's no longer about YAML or k8s, it's just about structural editors in general.",
        "user": "UP28ETUSE",
        "ts": "1595234226.035900",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "g7cefc64f7b1",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/7cefc64f7b1b53513625bf3487ecd16d.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
            "first_name": "Ionuț",
            "real_name": "Ionuț G. Stan",
            "display_name": "Ionuț G. Stan",
            "team": "T5TCAFTA9",
            "name": "ionut.g.stan",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "cG2",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Pulumi is very much like Terraform, it actually uses the Terraform providers behind the scenes (Pulumi does not generate files, as AWS CDK does). The only major difference is that you declare that desired state using an API of a general purpose language. But otherwise, Pulumi does exactly what "
                            },
                            {
                                "type": "user",
                                "user_id": "UJ6LDMMN0"
                            },
                            {
                                "type": "text",
                                "text": " said Terraform does — keeping that desired state, creating a plan, previewing it, applying it, etc.\n\nRegarding "
                            },
                            {
                                "type": "user",
                                "user_id": "UCUSW7WVD"
                            },
                            {
                                "type": "text",
                                "text": "'s problem, I think the solution in Pulumi would be similar to the one in Terrraform. Define a custom resource that does nothing than just block execution for an amount of time and then to the health check, then use this resource as a dependency of the second host you want to create.\n\nGoing back to the original question of whether it would make sense to have a visual editor for k8s config files, I don't think it would help that much. The problem with YAML is not that it's YAML, it's that it offers no means composition and abstraction. With only a visual editor that all it does is to offer affordances for YAML concepts, things will be the same. But, if that visual editors starts adding some PL concepts, such as variables and functions, then it can be useful, but at this point I think it's no longer about YAML or k8s, it's just about structural editors in general."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595084421.458800",
        "parent_user_id": "UKP3B2J5D",
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "UJ6LDMMN0",
                    "UKP3B2J5D"
                ],
                "count": 2
            }
        ]
    },
    {
        "client_msg_id": "b7a03e05-ecd3-41ae-85c7-80bf336a4763",
        "type": "message",
        "text": "I use the compiler tools from erlang, leex, yecc, absform and the compiler module",
        "user": "UBN9AFS0N",
        "ts": "1595234281.036100",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "7f0f1c0238ec",
            "image_72": "https:\/\/avatars.slack-edge.com\/2018-07-09\/395086754178_7f0f1c0238ec02befdab_72.jpg",
            "first_name": "Mariano",
            "real_name": "Mariano Guerra",
            "display_name": "",
            "team": "T5TCAFTA9",
            "name": "mariano",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "O7Gan",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I use the compiler tools from erlang, leex, yecc, absform and the compiler module"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595219570.025900",
        "parent_user_id": "U010328JA1E"
    },
    {
        "client_msg_id": "7921916b-71d1-4012-a121-812078ec2892",
        "type": "message",
        "text": "&gt; Are humans any different than machines?\n\nWe are. Our “model” does wetware based optimisation, morphing and most importantly localisation, which is something machines cant do while in hardware mode. Yeah, you can rearrange it in memory, but memory is 2D and that means huge overhead we don’t have. You need a multidimensional graph and mocking that in 2D is annoying as hell. And we are geared towards survival and reproduction, so we have “emotions” that reinforce our models\n\n&gt; Can we just continue to make better models and achieve understanding or consciousness or intelligence?\nNot this way. OpenAI has other stuff that is more geared toward better AI and intelligence. If we just continue to make better models but keep them shallow like this, only thing we will understand that intelligence isn’t pattern recognition.",
        "user": "UNBPP291C",
        "ts": "1595236276.036500",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gcf29e340a09",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/cf29e340a09ac08ae262632ef9101046.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0015-72.png",
            "first_name": "",
            "real_name": "Ian Rumac",
            "display_name": "Ian Rumac",
            "team": "T5TCAFTA9",
            "name": "ian.rumac",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "edited": {
            "user": "UNBPP291C",
            "ts": "1595236319.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Qd4yZ",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "> Are humans any different than machines?\n\nWe are. Our “model” does wetware based optimisation, morphing and most importantly localisation, which is something machines cant do while in hardware mode. Yeah, you can rearrange it in memory, but memory is 2D and that means huge overhead we don’t have. You need a multidimensional graph and mocking that in 2D is annoying as hell. And we are geared towards survival and reproduction, so we have “emotions” that reinforce our models\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_quote",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Can we just continue to make better models and achieve understanding or consciousness or intelligence?"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\nNot this way. OpenAI has other stuff that is more geared toward better AI and intelligence. If we just continue to make better models but keep them shallow like this, only thing we will understand that intelligence isn’t pattern recognition."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595144410.477000",
        "parent_user_id": "U0136G8R8KG"
    },
    {
        "client_msg_id": "c88fb355-8aea-404e-b66f-8efad5a7b8fd",
        "type": "message",
        "text": "I just use kotlin and wrote my own parser, the language feels so nice and expressive that I just keep coming back to it for everything.\nBasically just find special tokens, create a index of [position, token], then recursively parse it breadth first. tho it isn’t a complete “language”, but a DSL language so it’s easier than full on lang :)",
        "user": "UNBPP291C",
        "ts": "1595236571.036800",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gcf29e340a09",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/cf29e340a09ac08ae262632ef9101046.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0015-72.png",
            "first_name": "",
            "real_name": "Ian Rumac",
            "display_name": "Ian Rumac",
            "team": "T5TCAFTA9",
            "name": "ian.rumac",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "edited": {
            "user": "UNBPP291C",
            "ts": "1595236580.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Hb3M",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I just use kotlin and wrote my own parser, the language feels so nice and expressive that I just keep coming back to it for everything.\nBasically just find special tokens, create a index of [position, token], then recursively parse it breadth first. tho it isn’t a complete “language”, but a DSL language so it’s easier than full on lang :)"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595219570.025900",
        "parent_user_id": "U010328JA1E"
    },
    {
        "client_msg_id": "fdb4622d-e897-421d-996f-ecec3678dd52",
        "type": "message",
        "text": "and weird, nobody mentioned ANTLR yet",
        "user": "UNBPP291C",
        "ts": "1595236697.037100",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gcf29e340a09",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/cf29e340a09ac08ae262632ef9101046.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0015-72.png",
            "first_name": "",
            "real_name": "Ian Rumac",
            "display_name": "Ian Rumac",
            "team": "T5TCAFTA9",
            "name": "ian.rumac",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "8bP",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "and weird, nobody mentioned ANTLR yet"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595219570.025900",
        "parent_user_id": "U010328JA1E"
    },
    {
        "client_msg_id": "e3f1e2f3-171a-460c-8aa8-167bed515743",
        "type": "message",
        "text": "I've read the article last night, awesome stuff there. It's been a while since I also started thinking that codebase should be treated as a portmanteau of code + database, so this article was right up my alley. However, it never occurred to me that Datalog could be used for the database, even though I played a bit with Eve. I always thought of using a graph database, such as Neo4j.\n\n<@UHTPRR5SM> did you follow any resource for your Datalog implementation? Do you know of any tutorial-like resource that shows how to do that? I'm pretty familiar with unification I'd say, since I wrote a few Hindley-Milner type inference implementations, but I haven't yet come across a good exposition of how to write a Datalog interpreter.",
        "user": "UP28ETUSE",
        "ts": "1595237622.037300",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "g7cefc64f7b1",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/7cefc64f7b1b53513625bf3487ecd16d.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
            "first_name": "Ionuț",
            "real_name": "Ionuț G. Stan",
            "display_name": "Ionuț G. Stan",
            "team": "T5TCAFTA9",
            "name": "ionut.g.stan",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "edited": {
            "user": "UP28ETUSE",
            "ts": "1595238277.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "bv40p",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I've read the article last night, awesome stuff there. It's been a while since I also started thinking that codebase should be treated as a portmanteau of code + database, so this article was right up my alley. However, it never occurred to me that Datalog could be used for the database, even though I played a bit with Eve. I always thought of using a graph database, such as Neo4j.\n\n"
                            },
                            {
                                "type": "user",
                                "user_id": "UHTPRR5SM"
                            },
                            {
                                "type": "text",
                                "text": " did you follow any resource for your Datalog implementation? Do you know of any tutorial-like resource that shows how to do that? I'm pretty familiar with unification I'd say, since I wrote a few Hindley-Milner type inference implementations, but I haven't yet come across a good exposition of how to write a Datalog interpreter."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595084261.455800",
        "parent_user_id": "USH01JEDQ"
    },
    {
        "client_msg_id": "661bd06d-b34a-4326-9490-de63e7e3405c",
        "type": "message",
        "text": "<@UNBPP291C> I'd love to read some material about what you describe as \"wetware based optimization, morphing and localization\" — do you have any pointers to sources that go into depth on these?",
        "user": "U5STGTB3J",
        "ts": "1595239135.037700",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "649181907e06",
            "image_72": "https:\/\/avatars.slack-edge.com\/2017-08-20\/228447816352_649181907e06ec450c64_72.jpg",
            "first_name": "Stefan",
            "real_name": "Stefan Lesser",
            "display_name": "Stefan",
            "team": "T5TCAFTA9",
            "name": "stefanlesser",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "YuNP",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UNBPP291C"
                            },
                            {
                                "type": "text",
                                "text": " I'd love to read some material about what you describe as \"wetware based optimization, morphing and localization\" — do you have any pointers to sources that go into depth on these?"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595144410.477000",
        "parent_user_id": "U0136G8R8KG"
    },
    {
        "client_msg_id": "cef66e59-10e7-4bc3-987e-224a04b2fcc3",
        "type": "message",
        "text": "Let me look it up, I remember a paper last year that was about how brain uses spatial localisation to optimise (kinda annoying now Neural networks are muddying up my search results :joy:)\n\n By “optimization” I mean about constant retraining and optimisation our brains go through.\n\nOur “internal optimization agent” sees we have a  graph G(n) of n nodes and a graph G(m) of m nodes. They get combined into a graph of  k=(&lt;m*n) nodes, while maybe not retaining the absolute combined precision, but probably having a modulator which is extracted as a separate graph.\n\nFor example, Gn is trained for a fluid (oil)  and Gm is trained for oil. Then, they are merged\/splitted into a model that recognizes “fluid” pattern and a “viscosity” pattern (our brains are constantly looking for patterns. constantly. they are our primary source of optimization, i believe that this skill is critical to our brains evolution and that that is why geometry is important in our brains arrangements).  Let’s say you input lava, it would be recognized by both models, one recognizing it’s a fluid and the behaviors of fluid and one that can tell you the viscosity of it. Hope I explained it OK, will try to find papers, they do it better probs (i mean, this is my conclusion from reading multiple of papers, so its dot connecting conclusion from multiple sources).",
        "user": "UNBPP291C",
        "ts": "1595240115.037900",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gcf29e340a09",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/cf29e340a09ac08ae262632ef9101046.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0015-72.png",
            "first_name": "",
            "real_name": "Ian Rumac",
            "display_name": "Ian Rumac",
            "team": "T5TCAFTA9",
            "name": "ian.rumac",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "edited": {
            "user": "UNBPP291C",
            "ts": "1595243191.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "ix=cR",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Let me look it up, I remember a paper last year that was about how brain uses spatial localisation to optimise (kinda annoying now Neural networks are muddying up my search results "
                            },
                            {
                                "type": "emoji",
                                "name": "joy"
                            },
                            {
                                "type": "text",
                                "text": ")\n\n By “optimization” I mean about constant retraining and optimisation our brains go through.\n\nOur “internal optimization agent” sees we have a  graph G(n) of n nodes and a graph G(m) of m nodes. They get combined into a graph of  k=(<m*n) nodes, while maybe not retaining the absolute combined precision, but probably having a modulator which is extracted as a separate graph.\n\nFor example, Gn is trained for a fluid (oil)  and Gm is trained for oil. Then, they are merged\/splitted into a model that recognizes “fluid” pattern and a “viscosity” pattern (our brains are constantly looking for patterns. constantly. they are our primary source of optimization, i believe that this skill is critical to our brains evolution and that that is why geometry is important in our brains arrangements).  Let’s say you input lava, it would be recognized by both models, one recognizing it’s a fluid and the behaviors of fluid and one that can tell you the viscosity of it. Hope I explained it OK, will try to find papers, they do it better probs (i mean, this is my conclusion from reading multiple of papers, so its dot connecting conclusion from multiple sources)."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595144410.477000",
        "parent_user_id": "U0136G8R8KG"
    },
    {
        "client_msg_id": "36d1b47b-8d61-45c2-a797-677be0c1eac3",
        "type": "message",
        "text": "(<https:\/\/gtr.ukri.org\/projects?ref=MR%2FL009013%2F1#\/tabOverview> here is one that dabbed into that, but can’t find the one I wanted, an article is at <https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4598639\/> that is a nice intro)",
        "user": "UNBPP291C",
        "ts": "1595242880.039800",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gcf29e340a09",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/cf29e340a09ac08ae262632ef9101046.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0015-72.png",
            "first_name": "",
            "real_name": "Ian Rumac",
            "display_name": "Ian Rumac",
            "team": "T5TCAFTA9",
            "name": "ian.rumac",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "attachments": [
            {
                "title": "GtR",
                "title_link": "https:\/\/gtr.ukri.org\/projects?ref=MR%2FL009013%2F1#\/tabOverview",
                "text": "The Gateway to Research: UKRI portal onto publically funded research",
                "fallback": "GtR",
                "from_url": "https:\/\/gtr.ukri.org\/projects?ref=MR%2FL009013%2F1#\/tabOverview",
                "service_icon": "https:\/\/gtr.ukri.org\/favicon.ico",
                "service_name": "gtr.ukri.org",
                "id": 1,
                "original_url": "https:\/\/gtr.ukri.org\/projects?ref=MR%2FL009013%2F1#\/tabOverview"
            },
            {
                "service_name": "PubMed Central (PMC)",
                "title": "Brain Networks and Cognitive Architectures",
                "title_link": "https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4598639\/",
                "text": "Most accounts of human cognitive architectures have focused on computational accounts of cognition while making little contact with the study of anatomical structures and physiological processes. A renewed convergence between neurobiology and cognition ...",
                "fallback": "PubMed Central (PMC): Brain Networks and Cognitive Architectures",
                "thumb_url": "https:\/\/www.ncbi.nlm.nih.gov\/corehtml\/pmc\/pmcgifs\/pmc-logo-share.png",
                "from_url": "https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4598639\/",
                "thumb_width": 1200,
                "thumb_height": 630,
                "service_icon": "http:\/\/www.ncbi.nlm.nih.gov\/favicon.ico",
                "id": 2,
                "original_url": "https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4598639\/"
            }
        ],
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "P=K",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "("
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/gtr.ukri.org\/projects?ref=MR%2FL009013%2F1#\/tabOverview"
                            },
                            {
                                "type": "text",
                                "text": " here is one that dabbed into that, but can’t find the one I wanted, an article is at "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4598639\/"
                            },
                            {
                                "type": "text",
                                "text": " that is a nice intro)"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595144410.477000",
        "parent_user_id": "U0136G8R8KG",
        "reactions": [
            {
                "name": "pray::skin-tone-2",
                "users": [
                    "UP28ETUSE"
                ],
                "count": 1
            }
        ]
    },
    {
        "client_msg_id": "29885abb-454b-49c3-8e18-593d047c6f8f",
        "type": "message",
        "text": "Also, the more I read about GPT-3 it seems like:\n\n“Hey, remember that car we invented? Well, after making the engine a 1000 times bigger we get a lot more horsepower out of it!”",
        "user": "UNBPP291C",
        "ts": "1595243512.040200",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gcf29e340a09",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/cf29e340a09ac08ae262632ef9101046.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0015-72.png",
            "first_name": "",
            "real_name": "Ian Rumac",
            "display_name": "Ian Rumac",
            "team": "T5TCAFTA9",
            "name": "ian.rumac",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "edited": {
            "user": "UNBPP291C",
            "ts": "1595243521.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Z6KY",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Also, the more I read about GPT-3 it seems like:\n\n“Hey, remember that car we invented? Well, after making the engine a 1000 times bigger we get a lot more horsepower out of it!”"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595144410.477000",
        "parent_user_id": "U0136G8R8KG"
    },
    {
        "client_msg_id": "1B5615F0-9C60-4533-9661-00DCAA583FBA",
        "type": "message",
        "text": "<@UNBPP291C> Thanks for digging this up; looks promising. I have the same issue, often running into neural network stuff when searching for brain research.\n\nI’m specifically interested in categorization and how semantics manifest in our brains, schemas and frames, essentially the parts between the biochemistry of the brain and the philosophy of thought and reasoning — here’s a good overview from Lakoff: <https:\/\/youtu.be\/WuUnMCq-ARQ|https:\/\/youtu.be\/WuUnMCq-ARQ>\n\nIf you or anyone else comes across research in that area, please send it my way.",
        "user": "U5STGTB3J",
        "ts": "1595249972.046300",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "649181907e06",
            "image_72": "https:\/\/avatars.slack-edge.com\/2017-08-20\/228447816352_649181907e06ec450c64_72.jpg",
            "first_name": "Stefan",
            "real_name": "Stefan Lesser",
            "display_name": "Stefan",
            "team": "T5TCAFTA9",
            "name": "stefanlesser",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "attachments": [
            {
                "service_name": "YouTube",
                "service_url": "https:\/\/www.youtube.com\/",
                "title": "George Lakoff: How Brains Think: The Embodiment Hypothesis",
                "title_link": "https:\/\/youtu.be\/WuUnMCq-ARQ",
                "author_name": "PsychologicalScience",
                "author_link": "https:\/\/www.youtube.com\/user\/PsychologicalScience",
                "thumb_url": "https:\/\/i.ytimg.com\/vi\/WuUnMCq-ARQ\/hqdefault.jpg",
                "thumb_width": 480,
                "thumb_height": 360,
                "fallback": "YouTube Video: George Lakoff: How Brains Think: The Embodiment Hypothesis",
                "video_html": "<iframe width=\"400\" height=\"225\" src=\"https:\/\/www.youtube.com\/embed\/WuUnMCq-ARQ?feature=oembed&autoplay=1&iv_load_policy=3\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>",
                "video_html_width": 400,
                "video_html_height": 225,
                "from_url": "https:\/\/youtu.be\/WuUnMCq-ARQ",
                "service_icon": "https:\/\/a.slack-edge.com\/80588\/img\/unfurl_icons\/youtube.png",
                "id": 1,
                "original_url": "https:\/\/youtu.be\/WuUnMCq-ARQ"
            }
        ],
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "im2T8",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UNBPP291C"
                            },
                            {
                                "type": "text",
                                "text": " Thanks for digging this up; looks promising. I have the same issue, often running into neural network stuff when searching for brain research.\n\nI’m specifically interested in categorization and how semantics manifest in our brains, schemas and frames, essentially the parts between the biochemistry of the brain and the philosophy of thought and reasoning — here’s a good overview from Lakoff: "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/youtu.be\/WuUnMCq-ARQ",
                                "text": "https:\/\/youtu.be\/WuUnMCq-ARQ"
                            },
                            {
                                "type": "text",
                                "text": "\n\n"
                            },
                            {
                                "type": "text",
                                "text": "If you or anyone else comes across research in that area, please send it my way."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595144410.477000",
        "parent_user_id": "U0136G8R8KG",
        "reactions": [
            {
                "name": "pray::skin-tone-2",
                "users": [
                    "UP28ETUSE"
                ],
                "count": 1
            }
        ]
    },
    {
        "client_msg_id": "8fca77ce-1c79-4544-b8e8-d5cb743f8784",
        "type": "message",
        "text": "Reading the conversation in this thread kinda makes me wish this Slack also had a philosophy channel :slightly_smiling_face:",
        "user": "UP28ETUSE",
        "ts": "1595250167.046600",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "g7cefc64f7b1",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/7cefc64f7b1b53513625bf3487ecd16d.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
            "first_name": "Ionuț",
            "real_name": "Ionuț G. Stan",
            "display_name": "Ionuț G. Stan",
            "team": "T5TCAFTA9",
            "name": "ionut.g.stan",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Lz1UG",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Reading the conversation in this thread kinda makes me wish this Slack also had a philosophy channel "
                            },
                            {
                                "type": "emoji",
                                "name": "slightly_smiling_face"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595144410.477000",
        "parent_user_id": "U0136G8R8KG"
    },
    {
        "client_msg_id": "71e384cc-ed06-4f6c-8323-03d21ba2981a",
        "type": "message",
        "text": "<@U5STGTB3J> oooh interesting video, will check it out later when I have time! tldr as far as I see its about research that tries to find primitive model our brain recognizes and which are used as building blocks to build more advanced models by having a few types of interactions between them?",
        "user": "UNBPP291C",
        "ts": "1595252403.049200",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gcf29e340a09",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/cf29e340a09ac08ae262632ef9101046.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0015-72.png",
            "first_name": "",
            "real_name": "Ian Rumac",
            "display_name": "Ian Rumac",
            "team": "T5TCAFTA9",
            "name": "ian.rumac",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "EEDK",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "U5STGTB3J"
                            },
                            {
                                "type": "text",
                                "text": " oooh interesting video, will check it out later when I have time! tldr as far as I see its about research that tries to find primitive model our brain recognizes and which are used as building blocks to build more advanced models by having a few types of interactions between them?"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595144410.477000",
        "parent_user_id": "U0136G8R8KG"
    },
    {
        "client_msg_id": "C89676E8-90D4-40CA-9DAA-7A0ACBD41833",
        "type": "message",
        "text": "<@UNBPP291C> Yes, that describes the gist of it. It’s focused on language, which is only one of many parts of thinking — arguably quite an important one. Helps to be familiar with the work of George Lakoff and Mark Johnson on metaphorical structuring. There’ll be plenty of pointers in the lecture.",
        "user": "U5STGTB3J",
        "ts": "1595254913.053000",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "649181907e06",
            "image_72": "https:\/\/avatars.slack-edge.com\/2017-08-20\/228447816352_649181907e06ec450c64_72.jpg",
            "first_name": "Stefan",
            "real_name": "Stefan Lesser",
            "display_name": "Stefan",
            "team": "T5TCAFTA9",
            "name": "stefanlesser",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "edited": {
            "user": "U5STGTB3J",
            "ts": "1595254973.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "hcY",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UNBPP291C"
                            },
                            {
                                "type": "text",
                                "text": " Yes, that describes the gist of it. It’s focused on language, which is only one of many parts of thinking — arguably quite an important one. Helps to be familiar with the work of George Lakoff and Mark Johnson on metaphorical structuring. There’ll be plenty of pointers in the lecture."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595144410.477000",
        "parent_user_id": "U0136G8R8KG"
    },
    {
        "client_msg_id": "23f907fb-bb9d-4ec0-ac34-b895c59c0356",
        "type": "message",
        "text": "No clue who any of those people are but lecture sounds awesome, tnx for the link :thumbsup:",
        "user": "UNBPP291C",
        "ts": "1595254963.053200",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gcf29e340a09",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/cf29e340a09ac08ae262632ef9101046.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0015-72.png",
            "first_name": "",
            "real_name": "Ian Rumac",
            "display_name": "Ian Rumac",
            "team": "T5TCAFTA9",
            "name": "ian.rumac",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "haLP6",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "No clue who any of those people are but lecture sounds awesome, tnx for the link "
                            },
                            {
                                "type": "emoji",
                                "name": "thumbsup"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595144410.477000",
        "parent_user_id": "U0136G8R8KG"
    },
    {
        "client_msg_id": "38c9d1ff-c60d-4d92-9bb6-daccce145016",
        "type": "message",
        "text": "<https:\/\/medium.com\/the-long-now-foundation\/six-ways-to-think-long-term-da373b3377a4>\nprompt: how does this frame affect your thoughts about the future of programming?",
        "user": "UL3EE9WR1",
        "ts": "1595260067.054200",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gb6b8c893530",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/4b6b8c8935300db9089a9277d7e76a88.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0026-72.png",
            "first_name": "",
            "real_name": "Alex Miller",
            "display_name": "Alex Miller",
            "team": "T5TCAFTA9",
            "name": "alex678",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "attachments": [
            {
                "service_name": "Medium",
                "title": "Six Ways to Think Long-term",
                "title_link": "https:\/\/medium.com\/the-long-now-foundation\/six-ways-to-think-long-term-da373b3377a4",
                "text": "A Cognitive Toolkit for Good Ancestors",
                "fallback": "Medium: Six Ways to Think Long-term",
                "image_url": "https:\/\/miro.medium.com\/max\/1200\/1*SvVWXKySOzUGPqscVR4AaA.png",
                "fields": [
                    {
                        "title": "Reading time",
                        "value": "13 min read",
                        "short": true
                    }
                ],
                "ts": 1595255653,
                "from_url": "https:\/\/medium.com\/the-long-now-foundation\/six-ways-to-think-long-term-da373b3377a4",
                "image_width": 444,
                "image_height": 250,
                "image_bytes": 857602,
                "service_icon": "https:\/\/cdn-images-1.medium.com\/fit\/c\/152\/152\/1*8I-HPL0bfoIzGied-dzOvA.png",
                "id": 1,
                "original_url": "https:\/\/medium.com\/the-long-now-foundation\/six-ways-to-think-long-term-da373b3377a4"
            }
        ],
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "v2UD",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "link",
                                "url": "https:\/\/medium.com\/the-long-now-foundation\/six-ways-to-think-long-term-da373b3377a4"
                            },
                            {
                                "type": "text",
                                "text": "\nprompt: how does this frame affect your thoughts about the future of programming?"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595260067.054200",
        "reply_count": 4,
        "reply_users_count": 3,
        "latest_reply": "1595456799.186300",
        "reply_users": [
            "UNBPP291C",
            "UCUSW7WVD",
            "U01661S9F34"
        ],
        "replies": [
            {
                "user": "UNBPP291C",
                "ts": "1595266055.054900"
            },
            {
                "user": "UCUSW7WVD",
                "ts": "1595449802.178400"
            },
            {
                "user": "U01661S9F34",
                "ts": "1595451663.180900"
            },
            {
                "user": "UCUSW7WVD",
                "ts": "1595456799.186300"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "UCUSW7WVD",
                    "UJBAJNFLK"
                ],
                "count": 2
            }
        ]
    },
    {
        "client_msg_id": "325ebcea-41bd-4e73-946e-947d863ccb36",
        "type": "message",
        "text": "I hand rolled the lexer and parser for Curv in C++, because I didn't want to fight tool limitations in defining the syntax or implementing error recovery. In retrospect,\n• A regex-based scanner generator like re2c would have made the scanner easier to write.\n• My hand-written recursive descent parser was easy to write and continues to be easy to modify. No regrets.\n• After hanging out in FoC for a year, I have IDE envy. To properly implement completions and hints, I need an incremental parser. So maybe I should switch to the tree-sitter parser generator?  <https:\/\/github.com\/tree-sitter\/tree-sitter>\n• For me, the hard part is the back end, not the parser\/lexical analyser. What are the libraries\/DSLs for semantic analysis, optimization and code generation?",
        "user": "UJN1TAYEQ",
        "ts": "1595265341.054500",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "g4185a542241",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/34185a5422416f82b3e4a62964f2866b.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
            "first_name": "",
            "real_name": "Doug Moen",
            "display_name": "Doug Moen",
            "team": "T5TCAFTA9",
            "name": "doug",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "edited": {
            "user": "UJN1TAYEQ",
            "ts": "1595265352.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "38n",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I hand rolled the lexer and parser for Curv in C++, because I didn't want to fight tool limitations in defining the syntax or implementing error recovery. In retrospect,\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "A regex-based scanner generator like re2c would have made the scanner easier to write."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "My hand-written recursive descent parser was easy to write and continues to be easy to modify. No regrets."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "After hanging out in FoC for a year, I have IDE envy. To properly implement completions and hints, I need an incremental parser. So maybe I should switch to the tree-sitter parser generator?  "
                                    },
                                    {
                                        "type": "link",
                                        "url": "https:\/\/github.com\/tree-sitter\/tree-sitter"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "For me, the hard part is the back end, not the parser\/lexical analyser. What are the libraries\/DSLs for semantic analysis, optimization and code generation?"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0
                    }
                ]
            }
        ],
        "thread_ts": "1595219570.025900",
        "parent_user_id": "U010328JA1E",
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "U010328JA1E"
                ],
                "count": 1
            }
        ]
    },
    {
        "client_msg_id": "c8280e9f-d4eb-47af-be2b-7fb44b36e026",
        "type": "message",
        "text": "Re: affecting programming in this frame - less hype chasing, more deep knowledge, research and standardisation. The ego stance of “me and my ass” is affecting programming world a lot, especially through middle management.\n\nre: article - it’s surprisingly bad, 99 cent philosophy. I wrote a lenghty comment on HN about it, but I don’t think this article deserves even a minute more of my time considering how badly thought out it is. <https:\/\/news.ycombinator.com\/item?id=23899952>",
        "user": "UNBPP291C",
        "ts": "1595266055.054900",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "gcf29e340a09",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/cf29e340a09ac08ae262632ef9101046.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0015-72.png",
            "first_name": "",
            "real_name": "Ian Rumac",
            "display_name": "Ian Rumac",
            "team": "T5TCAFTA9",
            "name": "ian.rumac",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "edited": {
            "user": "UNBPP291C",
            "ts": "1595266119.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "YMAw",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Re: affecting programming in this frame - less hype chasing, more deep knowledge, research and standardisation. The ego stance of “me and my ass” is affecting programming world a lot, especially through middle management.\n\nre: article - it’s surprisingly bad, 99 cent philosophy. I wrote a lenghty comment on HN about it, but I don’t think this article deserves even a minute more of my time considering how badly thought out it is. "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/news.ycombinator.com\/item?id=23899952"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595260067.054200",
        "parent_user_id": "UL3EE9WR1"
    },
    {
        "client_msg_id": "761be2c5-5358-48c7-9667-5ecb8bb37b5d",
        "type": "message",
        "text": "<@UP28ETUSE> the main guide I had is a simple interpreter that a friend wrote, which isn’t open source. So, really the best I can offer up is my own implementation (simpleEvaluate.ts in the repo; not guaranteed to be bug free :stuck_out_tongue:) or these miniKanren implementations (a minimal logic programming language): <http:\/\/minikanren.org\/>",
        "user": "UHTPRR5SM",
        "ts": "1595269881.055300",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "3672059e546d",
            "image_72": "https:\/\/avatars.slack-edge.com\/2019-04-18\/614285184934_3672059e546d2aa66322_72.png",
            "first_name": "Pete",
            "real_name": "Pete Vilter",
            "display_name": "Pete Vilter",
            "team": "T5TCAFTA9",
            "name": "pete.vilter",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "RMUu",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UP28ETUSE"
                            },
                            {
                                "type": "text",
                                "text": " the main guide I had is a simple interpreter that a friend wrote, which isn’t open source. So, really the best I can offer up is my own implementation (simpleEvaluate.ts in the repo; not guaranteed to be bug free "
                            },
                            {
                                "type": "emoji",
                                "name": "stuck_out_tongue"
                            },
                            {
                                "type": "text",
                                "text": ") or these miniKanren implementations (a minimal logic programming language): "
                            },
                            {
                                "type": "link",
                                "url": "http:\/\/minikanren.org\/"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595084261.455800",
        "parent_user_id": "USH01JEDQ"
    },
    {
        "client_msg_id": "755aa9a5-204f-48d6-81e6-e0532d0ed141",
        "type": "message",
        "text": "I'd be curious if writing code in your native language is a net plus or net minus. Plus, use words you like and are used to. Minus, 70-90% of the world can't look at your code. If your code looks like this\n\n```関数 距離（頂点ー、頂点二）｛\n　返す 平方根（足す（二乗（頂点ー）二乗（頂点二）））\n｝```\nOnly people in your own language can read it. People not in that language can't even type it . If you ask for help you can only get that help from a much smaller set of people.\n\nAlso what do you do about libraries? Do you need a localized version of every library just so you can keep everything in your language?\n\nOf course I say this as a \"privileged\" English speaker. I have no idea what I would feel like using all these languages that are generally based in English if I wasn't fluent in English.\n\nAlso, I don't know about China or Korea but as an example when non-ASCII domain names became possible lots of people in Japan though kanji based domain names would take off. They didn't though. It's been 15+ years since it was possible but almost no companies use them.",
        "user": "UC6997THT",
        "ts": "1595304600.055800",
        "team": "T5TCAFTA9",
        "user_team": "T5TCAFTA9",
        "source_team": "T5TCAFTA9",
        "user_profile": {
            "avatar_hash": "f3eb3ca69d86",
            "image_72": "https:\/\/avatars.slack-edge.com\/2018-08-13\/414472553296_f3eb3ca69d86feb77929_72.png",
            "first_name": "Gregg",
            "real_name": "Gregg Tavares",
            "display_name": "gman",
            "team": "T5TCAFTA9",
            "name": "slack1",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "edited": {
            "user": "UC6997THT",
            "ts": "1595304629.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "349",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I'd be curious if writing code in your native language is a net plus or net minus. Plus, use words you like and are used to. Minus, 70-90% of the world can't look at your code. If your code looks like this\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_preformatted",
                        "elements": [
                            {
                                "type": "text",
                                "text": "関数 距離（頂点ー、頂点二）｛\n　返す 平方根（足す（二乗（頂点ー）二乗（頂点二）））\n｝"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Only people in your own language can read it. People not in that language can't even type it . If you ask for help you can only get that help from a much smaller set of people.\n\nAlso what do you do about libraries? Do you need a localized version of every library just so you can keep everything in your language?\n\nOf course I say this as a \"privileged\" English speaker. I have no idea what I would feel like using all these languages that are generally based in English if I wasn't fluent in English.\n\nAlso, I don't know about China or Korea but as an example when non-ASCII domain names became possible lots of people in Japan though kanji based domain names would take off. They didn't though. It's been 15+ years since it was possible but almost no companies use them."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1595138454.472900",
        "parent_user_id": "U0136G8R8KG"
    }
]