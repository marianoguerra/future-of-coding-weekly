[
    {
        "type": "message",
        "text": "Here's an angle: a few weeks playing in this _surprisingly visual_ language made years of reading about conditional probabilities finally click. I'm starting to feel like WebPPL is the language that we ought to teach probability with.\n\nWhile it's not a game, I find myself tweaking models just to see how it changes the generated charts. A WebPPL workflow is describing a probabilistic model (like \"I deal five cards and have two pair\") and returning values of interest (like \"whether the next deal has three-of-a-kind\"), which the system infers the likelihood of, and then presents using what it thinks are the best charts to suit the data. Its output is always a chart. Showering myself in random samples is more visceral, but seeing these plots was a rung on the ladder of abstraction that wasn't available to me before because the algebra made it so inaccessible.\n\nI've started to use WebPPL to interrogate my own cognition. The other day I asked whether my idea of \"healthy\" is just a function of how many calories is in a serving of food. I asked by comparing that to a model that just picked randomly\u2014turns out my theory was wrong, because the random model matched my tiny dataset better. Took me a couple of minutes. I don't even know how I would have posed that question before!\n\nI want to start using it in my art. Like, it's easy to explore the parameters of an L-system by changing the parameters, but I could work backwards by drawing an example of what I want, and getting a probability _distribution_ of parameters that could have generated it. It's a type of machine learning that feels far more human-scale, and more interesting than throwing gradient descent at classification and regression tasks.\n\nThe physicality of generative modeling, and of the related inference algorithms, are _begging_ to be embedding in a Hest-like interface!\n\nAnyway, I hope to be back with more later, hopefully in <#C03RR0W5DGC|devlog-together>. :)",
        "files": [
            {
                "id": "F06GC1FFCJX",
                "created": 1706643811,
                "timestamp": 1706643811,
                "name": "image.png",
                "title": "image.png",
                "mimetype": "image/png",
                "filetype": "png",
                "pretty_type": "PNG",
                "user": "UFEQUBNNT",
                "user_team": "T5TCAFTA9",
                "editable": false,
                "size": 80127,
                "mode": "hosted",
                "is_external": false,
                "external_type": "",
                "is_public": true,
                "public_url_shared": false,
                "display_as_bot": false,
                "username": "",
                "url_private": "https://files.slack.com/files-pri/T5TCAFTA9-F06GC1FFCJX/image.png",
                "url_private_download": "https://files.slack.com/files-pri/T5TCAFTA9-F06GC1FFCJX/download/image.png",
                "media_display_type": "unknown",
                "thumb_64": "https://files.slack.com/files-tmb/T5TCAFTA9-F06GC1FFCJX-e1352174a6/image_64.png",
                "thumb_80": "https://files.slack.com/files-tmb/T5TCAFTA9-F06GC1FFCJX-e1352174a6/image_80.png",
                "thumb_360": "https://files.slack.com/files-tmb/T5TCAFTA9-F06GC1FFCJX-e1352174a6/image_360.png",
                "thumb_360_w": 151,
                "thumb_360_h": 360,
                "thumb_480": "https://files.slack.com/files-tmb/T5TCAFTA9-F06GC1FFCJX-e1352174a6/image_480.png",
                "thumb_480_w": 202,
                "thumb_480_h": 480,
                "thumb_160": "https://files.slack.com/files-tmb/T5TCAFTA9-F06GC1FFCJX-e1352174a6/image_160.png",
                "thumb_720": "https://files.slack.com/files-tmb/T5TCAFTA9-F06GC1FFCJX-e1352174a6/image_720.png",
                "thumb_720_w": 302,
                "thumb_720_h": 720,
                "thumb_800": "https://files.slack.com/files-tmb/T5TCAFTA9-F06GC1FFCJX-e1352174a6/image_800.png",
                "thumb_800_w": 336,
                "thumb_800_h": 800,
                "thumb_960": "https://files.slack.com/files-tmb/T5TCAFTA9-F06GC1FFCJX-e1352174a6/image_960.png",
                "thumb_960_w": 403,
                "thumb_960_h": 960,
                "thumb_1024": "https://files.slack.com/files-tmb/T5TCAFTA9-F06GC1FFCJX-e1352174a6/image_1024.png",
                "thumb_1024_w": 430,
                "thumb_1024_h": 1024,
                "original_w": 536,
                "original_h": 1276,
                "thumb_tiny": "AwAwABS/na4X1p9QSHY6k84yanoAKKKKAI5EDYyM9akpvXAI5paAFopKKAGjk89adSYOSaXn0oAKKPwpMe1AH//Z",
                "permalink": "https://futureofcoding.slack.com/files/UFEQUBNNT/F06GC1FFCJX/image.png",
                "permalink_public": "https://slack-files.com/T5TCAFTA9-F06GC1FFCJX-ae26b30ab9",
                "is_starred": false,
                "has_rich_preview": false,
                "file_access": "visible"
            }
        ],
        "upload": false,
        "user": "UFEQUBNNT",
        "display_as_bot": false,
        "ts": "1706645536.535439",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "fdHVW",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Here's an angle: a few weeks playing in this "
                            },
                            {
                                "type": "text",
                                "text": "surprisingly visual",
                                "style": {
                                    "bold": false,
                                    "italic": true,
                                    "strike": false
                                }
                            },
                            {
                                "type": "text",
                                "text": " language made years of reading about conditional probabilities finally click. I'm starting to feel like WebPPL is the language that we ought to teach probability with.\n\nWhile it's not a game, I find myself tweaking models just to see how it changes the generated charts. A WebPPL workflow is describing a probabilistic model (like \"I deal five cards and have two pair\") and returning values of interest (like \"whether the next deal has three-of-a-kind\"), which the system infers the likelihood of, and then presents using what it thinks are the best charts to suit the data. Its output is always a chart. Showering myself in random samples is more visceral, but seeing these plots was a rung on the ladder of abstraction that wasn't available to me before because the algebra made it so inaccessible.\n\nI've started to use WebPPL to interrogate my own cognition. The other day I asked whether my idea of \"healthy\" is just a function of how many calories is in a serving of food. I asked by comparing that to a model that just picked randomly\u2014turns out my theory was wrong, because the random model matched my tiny dataset better. Took me a couple of minutes. I don't even know how I would have posed that question before!\n\nI want to start using it in my art. Like, it's easy to explore the parameters of an L-system by changing the parameters, but I could work backwards by drawing an example of what I want, and getting a probability "
                            },
                            {
                                "type": "text",
                                "text": "distribution",
                                "style": {
                                    "bold": false,
                                    "italic": true,
                                    "strike": false
                                }
                            },
                            {
                                "type": "text",
                                "text": " of parameters that could have generated it. It's a type of machine learning that feels far more human-scale, and more interesting than throwing gradient descent at classification and regression tasks.\n\nThe physicality of generative modeling, and of the related inference algorithms, are "
                            },
                            {
                                "type": "text",
                                "text": "begging",
                                "style": {
                                    "bold": false,
                                    "italic": true,
                                    "strike": false
                                }
                            },
                            {
                                "type": "text",
                                "text": " to be embedding in a Hest-like interface!\n\nAnyway, I hope to be back with more later, hopefully in "
                            },
                            {
                                "type": "channel",
                                "channel_id": "C03RR0W5DGC"
                            },
                            {
                                "type": "text",
                                "text": ". :)"
                            }
                        ]
                    }
                ]
            }
        ],
        "edited": {
            "user": "UFEQUBNNT",
            "ts": "1706652858.000000"
        },
        "client_msg_id": "8a339c4c-eee6-4fa3-8a99-fc4913bb96fa",
        "thread_ts": "1705768349.209529",
        "parent_user_id": "UFEQUBNNT",
        "subtype": "thread_broadcast",
        "root": {
            "client_msg_id": "26717735-D583-47BE-AA29-80D927BBC5A2",
            "type": "message",
            "text": "I\u2019ve been really into probabilistic programming languages (PPLs) lately. I had a few \u201caha\u201d moments and I\u2019m decidedly in the \u201chit everything with this hammer to figure out where the nails are\u201d phase.\n\nAnyway, is anybody here in a similar headspace? Or already experienced with PPLs?\n\nIs anybody curious to enter this headspace with me? I reviewed the (online, free, programmable) book that brought me here: <https://micro.alltom.com/2024/01/11/the-cognition-in.html|https://micro.alltom.com/2024/01/11/the-cognition-in.html>",
            "user": "UFEQUBNNT",
            "ts": "1705768349.209529",
            "blocks": [
                {
                    "type": "rich_text",
                    "block_id": "NTjNv",
                    "elements": [
                        {
                            "type": "rich_text_section",
                            "elements": [
                                {
                                    "type": "text",
                                    "text": "I\u2019ve been really into probabilistic programming languages (PPLs) lately. I had a few \u201caha\u201d moments and I\u2019m decidedly in the \u201chit everything with this hammer to figure out where the nails are\u201d phase.\n\nAnyway, is anybody here in a similar headspace? Or already experienced with PPLs?\n\nIs anybody curious to enter this headspace with me? I reviewed the (online, free, programmable) book that brought me here: "
                                },
                                {
                                    "type": "link",
                                    "url": "https://micro.alltom.com/2024/01/11/the-cognition-in.html",
                                    "text": "https://micro.alltom.com/2024/01/11/the-cognition-in.html"
                                }
                            ]
                        }
                    ]
                }
            ],
            "team": "T5TCAFTA9",
            "attachments": [
                {
                    "from_url": "https://micro.alltom.com/2024/01/11/the-cognition-in.html",
                    "id": 1,
                    "original_url": "https://micro.alltom.com/2024/01/11/the-cognition-in.html",
                    "fallback": "Tom Lieber -",
                    "text": "The \u201cCognition\u201d in Probabilistic Models of Cognition :books: is the hardest part to get across when I recommend this book. Because the book isn\u2019t \u201cBayes works, yawn.\u201d It\u2019s so much more interesting if you, like me, exist primarily in ML circles that focus on curve-fitting:\n What could be going on in our heads when we can\u2019t tell that the two grid squares in that optical illusion are the same color?",
                    "title": "Tom Lieber -",
                    "title_link": "https://micro.alltom.com/2024/01/11/the-cognition-in.html",
                    "service_name": "micro.alltom.com"
                }
            ],
            "thread_ts": "1705768349.209529",
            "reply_count": 3,
            "reply_users_count": 2,
            "latest_reply": "1706664792.439119",
            "reply_users": [
                "UFEQUBNNT",
                "UAJKEBGP8"
            ],
            "is_locked": false,
            "subscribed": false
        },
        "reactions": [
            {
                "name": "heartbeat",
                "users": [
                    "UCUSW7WVD",
                    "U05UK5T7LPP"
                ],
                "count": 2
            },
            {
                "name": "slot_machine",
                "users": [
                    "U02QC0PPER3"
                ],
                "count": 1
            }
        ]
    }
]