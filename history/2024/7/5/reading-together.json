[
    {
        "user": "U07BWTYKJQG",
        "type": "message",
        "ts": "1720203029.699949",
        "client_msg_id": "eba4129f-3758-45a8-b1d0-3881ba993e2d",
        "text": "I've been kinda obsessed with this paper, \"Weird Machines, Exploitability, and Provable Unexploitability\" since October last year.\n<https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8226852>\n<https://www.youtube.com/watch?v=1ynkWcfiwOk>\n(I think starting with the talk is a good idea)\n\nAfter hearing about robust first computing I've been pondering the connection for a bit. I'm not really sure what to make of it, but there has to be some connection. In <https://www.youtube.com/watch?v=Dmlm6mtnSZs|this talk> by Dave Ackley he specifically mentions that \"flipping one bit\" (paraphrasing) can be enough to destroy a lot of software, which is the exact attack model that Dullien uses. It would be nice if software could maintain security invariants despite memory corruption (which Dullien proves for a small piece of software that he writes) so in that sense the two concepts allign. But I find that \"continuing despite an error\" or \"return a best-effort answer\" is the source of many security vulnerabilities, (mid example: database lookup fails, code is written in a way that forgot to handle that case, code continues anyway and returns admin data that shouldn't be shown). I wonder if any of y'all have read this or have thoughts about it. :))",
        "team": "T5TCAFTA9",
        "thread_ts": "1720203029.699949",
        "reply_count": 3,
        "reply_users_count": 3,
        "latest_reply": "1720360934.437349",
        "reply_users": [
            "UCUSW7WVD",
            "U07BWTYKJQG",
            "U07AR9MFUUF"
        ],
        "is_locked": false,
        "subscribed": false,
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "ItZ/+",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I've been kinda obsessed with this paper, \"Weird Machines, Exploitability, and Provable Unexploitability\" since October last year.\n"
                            },
                            {
                                "type": "link",
                                "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8226852"
                            },
                            {
                                "type": "text",
                                "text": "\n"
                            },
                            {
                                "type": "link",
                                "url": "https://www.youtube.com/watch?v=1ynkWcfiwOk"
                            },
                            {
                                "type": "text",
                                "text": "\n(I think starting with the talk is a good idea)\n\nAfter hearing about robust first computing I've been pondering the connection for a bit. I'm not really sure what to make of it, but there has to be some connection. In "
                            },
                            {
                                "type": "link",
                                "url": "https://www.youtube.com/watch?v=Dmlm6mtnSZs",
                                "text": "this talk"
                            },
                            {
                                "type": "text",
                                "text": " by Dave Ackley he specifically mentions that \"flipping one bit\" (paraphrasing) can be enough to destroy a lot of software, which is the exact attack model that Dullien uses. It would be nice if software could maintain security invariants despite memory corruption (which Dullien proves for a small piece of software that he writes) so in that sense the two concepts allign. But I find that \"continuing despite an error\" or \"return a best-effort answer\" is the source of many security vulnerabilities, (mid example: database lookup fails, code is written in a way that forgot to handle that case, code continues anyway and returns admin data that shouldn't be shown). I wonder if any of y'all have read this or have thoughts about it. :))"
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "heartbeat",
                "users": [
                    "UCUSW7WVD"
                ],
                "count": 1
            }
        ]
    },
    {
        "user": "UCUSW7WVD",
        "type": "message",
        "ts": "1720221975.552169",
        "client_msg_id": "97dc8732-14e0-4244-a7ee-77bc71e3a0c6",
        "text": "I'm still thinking about this one, but want to share something I wrote a few years that starts out from a similar framing: <https://akkartik.name/post/modularity>\n\nOP is much cleaner. What I refer to fuzzily as \"territories of programs\", OP makes precise as the distinction between the intended finite state machine and its _emulation_ on a (fixed-memory, so you end up padding a bunch of don't-care bits in the address space) real computer.",
        "team": "T5TCAFTA9",
        "thread_ts": "1720203029.699949",
        "parent_user_id": "U07BWTYKJQG",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "eQO1w",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I'm still thinking about this one, but want to share something I wrote a few years that starts out from a similar framing: "
                            },
                            {
                                "type": "link",
                                "url": "https://akkartik.name/post/modularity"
                            },
                            {
                                "type": "text",
                                "text": "\n\nOP is much cleaner. What I refer to fuzzily as \"territories of programs\", OP makes precise as the distinction between the intended finite state machine and its "
                            },
                            {
                                "type": "text",
                                "text": "emulation",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " on a (fixed-memory, so you end up padding a bunch of don't-care bits in the address space) real computer."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U07BWTYKJQG",
        "type": "message",
        "ts": "1720254838.939829",
        "edited": {
            "user": "U07BWTYKJQG",
            "ts": "1720255114.000000"
        },
        "client_msg_id": "4f3ecb3a-9a3e-422a-8f88-d34eb376f719",
        "text": "Ah, I see the connection! And I see how tests are a good aid for this. My take on tests is that everyone who cares about their software being correct should do them, because I don't believe anyone can make and maintain correct software, so tests will be useful to show how and where you fail. But also they don't give correctness under typical use and definitely not security (read: correctness) under adversarial conditions. And and they don't give robustness, but also i'm not quite sure what robustness is really but i think my software should have it\n\n>  OP is much cleaner\nMore formal at least, your writing is very clear.\n\nI feel like there's a tension between formal correctness and robustness. The whole \"correctness and efficiency only\" goal of current software methods means we don't think about what happens after faults or memory vulnerabilities. I think I'd like to have correctness despite faults, which is different from what Robust-first computing seems to do? I'm not sure",
        "team": "T5TCAFTA9",
        "thread_ts": "1720203029.699949",
        "parent_user_id": "U07BWTYKJQG",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "WgbtO",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Ah, I see the connection! And I see how tests are a good aid for this. My take on tests is that everyone who cares about their software being correct should do them, because I don't believe anyone can make and maintain correct software, so tests will be useful to show how and where you fail. But also they don't give correctness under typical use and definitely not security (read: correctness) under adversarial conditions. And and they don't give robustness, but also i'm not quite sure what robustness is really but i think my software should have it\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_quote",
                        "elements": [
                            {
                                "type": "text",
                                "text": " OP is much cleaner"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "More formal at least, your writing is very clear.\n\nI feel like there's a tension between formal correctness and robustness. The whole \"correctness and efficiency only\" goal of current software methods means we don't think about what happens after faults or memory vulnerabilities. I think I'd like to have correctness despite faults, which is different from what Robust-first computing seems to do? I'm not sure"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U07AR9MFUUF",
        "type": "message",
        "ts": "1720360934.437349",
        "client_msg_id": "513d08f1-61b2-4354-bdc9-a3f232b0ca69",
        "text": "All I know about robust computing comes from the Future of Coding episode on it, so this response is probably steeped in ignorance. One thing not mentioned in the episode is probabilistic algorithms. The usual setup is to assume a deterministic computer, and then introduce imperfect information (often via a hash function) giving a small probability of error but achieving a large reduction in time or space requirements. Perhaps the most used algorithm is [HyperLogLog](<https://en.wikipedia.org/wiki/HyperLogLog>), but I think [MinHash](<https://en.wikipedia.org/wiki/MinHash>) is particularly easy to understand. This isn't the same as assuming any part of the algorithm can fail, but it might provide a pathway to that model.",
        "team": "T5TCAFTA9",
        "thread_ts": "1720203029.699949",
        "parent_user_id": "U07BWTYKJQG",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "gpbhS",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "All I know about robust computing comes from the Future of Coding episode on it, so this response is probably steeped in ignorance. One thing not mentioned in the episode is probabilistic algorithms. The usual setup is to assume a deterministic computer, and then introduce imperfect information (often via a hash function) giving a small probability of error but achieving a large reduction in time or space requirements. Perhaps the most used algorithm is [HyperLogLog]("
                            },
                            {
                                "type": "link",
                                "url": "https://en.wikipedia.org/wiki/HyperLogLog"
                            },
                            {
                                "type": "text",
                                "text": "), but I think [MinHash]("
                            },
                            {
                                "type": "link",
                                "url": "https://en.wikipedia.org/wiki/MinHash"
                            },
                            {
                                "type": "text",
                                "text": ") is particularly easy to understand. This isn't the same as assuming any part of the algorithm can fail, but it might provide a pathway to that model."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]